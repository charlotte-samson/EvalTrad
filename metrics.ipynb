{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports généraux\n",
    "from __future__ import division\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "# imports pour le modèle de langue\n",
    "import kenlm\n",
    "# imports pour ibm1\n",
    "import nltk\n",
    "from nltk.translate import AlignedSent, IBMModel1\n",
    "from nltk.tokenize import word_tokenize\n",
    "# imports pour bleu et meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from operator import attrgetter\n",
    "#imports pour camemBERT\n",
    "from fairseq.models.roberta import CamembertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(ressource_file, maxnbsent):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    corpus = json.load(open(ressource_file)) # liste de dictionnaires (src,hyp,ref,score)\n",
    "    \n",
    "    totalnbsent = 0        \n",
    "    for i,sentence in enumerate(corpus) : # sentence = { src = \"\" , hyp = \"\" , ref = \"\", score = \"\" }\n",
    "        totalnbsent +=1\n",
    "\n",
    "        data.append({ \n",
    "                            \"idx\" : i,\n",
    "                            \"src\" : nltk.word_tokenize(sentence['src']), \n",
    "                            \"hyp\" : nltk.word_tokenize(sentence['hyp']), \n",
    "                            \"ref\" : nltk.word_tokenize(sentence['ref']), \n",
    "                            \"score\" : round(sentence[\"score\"],2), \n",
    "                            \"model\" : None,\n",
    "                            \"ibm1\" : None,\n",
    "                            \"bleu\" : None, \n",
    "                            \"meteor\" : None,\n",
    "                            \"cos\" : None\n",
    "                        })\n",
    "\n",
    "        if maxnbsent > 0 and len(data) >= maxnbsent:\n",
    "            print( str(len(data))+' phrases chargees')\n",
    "            return data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm1_score_sentence(e, f) :\n",
    "        proba = 1\n",
    "        for wf in f :\n",
    "            temp = 0\n",
    "            for we in e :\n",
    "                temp += ibm1.translation_table[we][wf]\n",
    "            proba = proba*temp\n",
    "        return ((1/((len(e)+1)**len(f))) * proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(sentence):\n",
    "    #obtenir l'embeddings d'une phrase\n",
    "    tokens = camembert.encode(sentence)\n",
    "    all_layers = camembert.extract_features(tokens, return_all_hiddens=True)\n",
    "    #on prend l'avant dernier layer (on pourrait en choisir un autre)\n",
    "    pooling_layer = all_layers[-2] \n",
    "    #moyenne des vecteurs pour s'affranchir de la longueur des phrases\n",
    "    embedded = pooling_layer.mean(1) \n",
    "    return embedded\n",
    "\n",
    "def score_camembert(sentA,sentB):\n",
    "    #renvoie la cosinus similarité entre 2 phrases\n",
    "    vec_1= embeddings(sentA).detach().numpy().squeeze()\n",
    "    vec_2 = embeddings(sentB).detach().numpy().squeeze()\n",
    "    simcos = np.dot(vec_1,vec_2) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
    "    return simcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "999 phrases chargees\nloading archive file ../camembert.v0\n| dictionary: 32004 types\n"
    },
    {
     "data": {
      "text/plain": "RobertaHubInterface(\n  (model): RobertaModel(\n    (decoder): RobertaEncoder(\n      (sentence_encoder): TransformerSentenceEncoder(\n        (embed_tokens): Embedding(32005, 768, padding_idx=1)\n        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n        (layers): ModuleList(\n          (0): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): TransformerSentenceEncoderLayer(\n            (self_attn): MultiheadAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): RobertaLMHead(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (classification_heads): ModuleDict()\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_data(\"../ressources/en2fr_manual_evaluation.json\",999)\n",
    "\n",
    "model = kenlm.LanguageModel('../ressources/europarl.binary')\n",
    "ibm1 = IBMModel1([AlignedSent(d['src'],d['hyp']) for d in data], 20)\n",
    "bleu_sf = SmoothingFunction()\n",
    "camembert = CamembertModel.from_pretrained('../camembert.v0')\n",
    "camembert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}\n",
    "for d in data :\n",
    "    d['model'] = math.pow(10,model.score(\" \".join(d[\"hyp\"])))\n",
    "    d['ibm1'] = ibm1_score_sentence(d['src'],d['hyp'])\n",
    "    d['bleu'] = nltk.translate.bleu_score.sentence_bleu(d[\"ref\"],d[\"hyp\"],smoothing_function= bleu_sf.method2)\n",
    "    d['meteor'] = nltk.translate.meteor_score.meteor_score(\" \".join(d[\"ref\"]),\" \".join(d[\"hyp\"]))\n",
    "    d['cos'] = score_camembert(\" \".join(d[\"ref\"]),\" \".join(d[\"hyp\"]))\n",
    "    final_dict[d['idx']] = [\" \".join(d['src']),\" \".join(d['hyp']),\" \".join(d['ref']),d['score'],d['model'],d['ibm1'],d['bleu'],d['meteor'],d['cos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(final_dict,orient='index')\n",
    "df.columns=[\"src\",\"hyp\",\"ref\",\"score_humain\",\"model\",\"ibm1\",\"bleu\",\"meteor\",\"cos\"]\n",
    "df.to_csv(\"../ressources/metrics.csv\")"
   ]
  }
 ]
}